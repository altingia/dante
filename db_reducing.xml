<tool id="db_reducing" name="ProfRep DB Reducing" version="1.0.0">
<description> Tool to reduce database of reads sequences based on the reads similarity in order to speed up ProfRep analysis </description>
<requirements>
    <requirement type="package" version="4.6.4" >cd-hit</requirement>
</requirements>
<command>
python3 ${__tool_directory__}/profrep_db_reducing.py --reads_all ${reads_all} --cls ${cls} --cluster_size ${cluster_size} --identity_th ${identity_th} --reads_reduced ${reads_reduced} --cls_reduced ${cls_reduced}
</command>
<inputs>
 <param format="fasta" type="data" name="reads_all" label="NGS reads" help="Choose input file containing all reads sequences to be reduced" />
 <param format="fasta" type="data" name="cls" label="RE clusters" help="Choose file containing all clusters and belonging reads [ RE archive -> seqclust -> clustering -> hitsort.cls" />
 <param name="cluster_size" type="integer" value="1000" min="1" max ="1000000000" label="Min cluster size" help="Only the reads from most represented clusters will be reduced - parameter indicates min. number of reads in a cluster to be involved in reducing" />
 <param name="identity_th" type="float" value="0.90" min="0.1" max ="1.0" label="Reads identity threshold" help="Percentage of identity between reads sequences to group and reduce them" />
</inputs>

<outputs>
 <data format="fasta" name="cls_reduced" label="Modified cls file of ${cls.hid}" />
 <data format="fasta" name="reads_reduced" label="Reduced reads database of ${reads_all.hid}" />
</outputs>

 <help>

**WHAT IT DOES**

This tool will reduce the database of all reads based on similarities between them. Basically, it creates clusters of similar reads and the reduced database will then be composed of one representative read for all from the cluster, also indicating the number of reads that it represents. As the new reads database is produced, CLS file containing reads connected to clusters has to be modified as well. The actual reduction level depends on number of clusters envolved and how big they are. Default value for cluster size to be involved in reducing is 1000, which means all clusters containing 1000 and more reads are going to be reduced. 

 </help>
</tool>




